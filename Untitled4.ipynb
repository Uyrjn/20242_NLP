{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205146b2-e75c-4542-8295-27484578ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForTokenClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import get_scheduler\n",
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf476a8-1770-4775-93d3-9420592dfde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Train.csv')\n",
    "\n",
    "sentences = []\n",
    "aspect_terms = []\n",
    "\n",
    "for sent_id, group in df.groupby('id'):\n",
    "    sent = group['Sentence'].iloc[0]\n",
    "    aspects = group['Aspect Term'].tolist()\n",
    "    sentences.append(sent)\n",
    "    aspect_terms.append(aspects)\n",
    "\n",
    "# --- 3. Initialize tokenizer ---\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# --- 4. Create BIO labels ---\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = []\n",
    "    idx = 0\n",
    "    for word_ids in encodings.word_ids():\n",
    "        if word_ids is None:\n",
    "            labels.append(-100)  \n",
    "        else:\n",
    "            labels.append(tags[word_ids])\n",
    "    return labels\n",
    "\n",
    "def get_bio_tags(sentence, aspects):\n",
    "    words = sentence.split()\n",
    "    tags = ['O'] * len(words)\n",
    "\n",
    "    for asp in aspects:\n",
    "        asp_words = asp.split()\n",
    "        for i in range(len(words) - len(asp_words) + 1):\n",
    "            if words[i:i+len(asp_words)] == asp_words:\n",
    "                tags[i] = 'B-ASP'\n",
    "                for j in range(1, len(asp_words)):\n",
    "                    tags[i+j] = 'I-ASP'\n",
    "\n",
    "    tag2id = {'O':0, 'B-ASP':1, 'I-ASP':2}\n",
    "    tag_ids = [tag2id[tag] for tag in tags]\n",
    "    return tag_ids\n",
    "\n",
    "# --- 5. Prepare dataset class ---\n",
    "class AspectTermDataset(Dataset):\n",
    "    def __init__(self, sentences, aspect_terms, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.aspect_terms = aspect_terms\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = {'O':0, 'B-ASP':1, 'I-ASP':2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        aspects = self.aspect_terms[idx]\n",
    "        words = sentence.split()\n",
    "\n",
    "        tags = ['O'] * len(words)\n",
    "        for asp in aspects:\n",
    "            asp_words = asp.split()\n",
    "            for i in range(len(words) - len(asp_words) + 1):\n",
    "                if words[i:i+len(asp_words)] == asp_words:\n",
    "                    tags[i] = 'B-ASP'\n",
    "                    for j in range(1, len(asp_words)):\n",
    "                        tags[i+j] = 'I-ASP'\n",
    "        encoding = self.tokenizer(sentence.split(),\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=128,\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "        labels = []\n",
    "        word_ids = encoding.word_ids(batch_index=0)  \n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)  \n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(self.tag2id[tags[word_idx]])\n",
    "            else:\n",
    "                labels.append(self.tag2id[tags[word_idx]] if tags[word_idx].startswith('I') else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        return item\n",
    "\n",
    "# --- 6. Prepare train/test split ---\n",
    "train_sentences, val_sentences, train_aspects, val_aspects = train_test_split(\n",
    "    sentences, aspect_terms, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = AspectTermDataset(train_sentences, train_aspects, tokenizer)\n",
    "val_dataset = AspectTermDataset(val_sentences, val_aspects, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9060b4e7-6280-48c7-913e-2ad3d5493df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|███████████████| 152/152 [32:44<00:00, 12.92s/it, loss=0.225]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Avg Loss: 0.2029 - Accuracy: 0.9249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Chuẩn bị device ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- 2. Load model ---\n",
    "num_labels = 3  # Ví dụ 3 nhãn: B, I, O\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "model.to(device)\n",
    "\n",
    "# --- 3. Optimizer và scheduler ---\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# --- 4. Training loop ---\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'offset_mapping'}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = outputs.logits \n",
    "        preds = torch.argmax(logits, dim=-1)  \n",
    "        labels = batch[\"labels\"]  \n",
    "        mask = labels != -100\n",
    "        preds = preds[mask].detach().cpu().numpy()\n",
    "        labels = labels[mask].detach().cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch {epoch+1} - Avg Loss: {avg_loss:.4f} - Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da83b488-e2c2-471a-a3aa-921370befabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The acting was phenomenal, but the storyline was predictable.\n",
      "Extracted Aspects: ['acting', 'storyline']\n",
      "------------------------------------------------------------\n",
      "Sentence: I loved the cinematography, but the pacing was too slow.\n",
      "Extracted Aspects: ['cinematography', 'pacing']\n",
      "------------------------------------------------------------\n",
      "Sentence: The soundtrack was amazing and the dialogue felt natural.\n",
      "Extracted Aspects: ['soundtrack', 'dialogue']\n",
      "------------------------------------------------------------\n",
      "Sentence: The plot twists were unexpected, but the ending was disappointing.\n",
      "Extracted Aspects: ['plot twists', 'ending']\n",
      "------------------------------------------------------------\n",
      "Sentence: The characters were well-developed and very relatable.\n",
      "Extracted Aspects: ['characters']\n",
      "------------------------------------------------------------\n",
      "Sentence: The visual effects were stunning, but the script was weak.\n",
      "Extracted Aspects: ['visual effects', 'script']\n",
      "------------------------------------------------------------\n",
      "Sentence: The movie had a great message, but the direction lacked focus.\n",
      "Extracted Aspects: ['direction']\n",
      "------------------------------------------------------------\n",
      "Sentence: The performance by the lead actor was top-notch.\n",
      "Extracted Aspects: ['performance']\n",
      "------------------------------------------------------------\n",
      "Sentence: The editing was smooth and the scenes transitioned well.\n",
      "Extracted Aspects: ['editing', 'scenes']\n",
      "------------------------------------------------------------\n",
      "Sentence: The humor was forced, and the romance felt unnecessary.\n",
      "Extracted Aspects: ['humor', 'romance']\n",
      "------------------------------------------------------------\n",
      "Sentence: The battery life of this smartphone is incredible, but the camera quality is mediocre.\n",
      "Extracted Aspects: ['battery life', 'camera quality']\n",
      "------------------------------------------------------------\n",
      "Sentence: I really appreciate the fast processor, but the device heats up quickly.\n",
      "Extracted Aspects: []\n",
      "------------------------------------------------------------\n",
      "Sentence: The screen resolution is crystal clear, yet the speaker sound is disappointing.\n",
      "Extracted Aspects: ['screen resolution', 'speaker sound']\n",
      "------------------------------------------------------------\n",
      "Sentence: Charging is fast but the charger cable feels fragile.\n",
      "Extracted Aspects: ['charging', 'charger cable']\n",
      "------------------------------------------------------------\n",
      "Sentence: The software update fixed many bugs but introduced new ones.\n",
      "Extracted Aspects: ['software update']\n",
      "------------------------------------------------------------\n",
      "Sentence: The customer service was very helpful and resolved my issue quickly.\n",
      "Extracted Aspects: ['customer service']\n",
      "------------------------------------------------------------\n",
      "Sentence: Waiting time was too long, but the representative was polite.\n",
      "Extracted Aspects: ['waiting time', 'representative']\n",
      "------------------------------------------------------------\n",
      "Sentence: They responded promptly, but the solution was not satisfactory.\n",
      "Extracted Aspects: ['solution']\n",
      "------------------------------------------------------------\n",
      "Sentence: The support team was unprofessional and rude.\n",
      "Extracted Aspects: ['support team']\n",
      "------------------------------------------------------------\n",
      "Sentence: I appreciated the follow-up calls after the purchase.\n",
      "Extracted Aspects: []\n",
      "------------------------------------------------------------\n",
      "Sentence: The pizza crust was crispy and delicious, but the toppings were sparse.\n",
      "Extracted Aspects: ['pizza crust', 'toppings']\n",
      "------------------------------------------------------------\n",
      "Sentence: Service was quick, but the waiter forgot our drinks.\n",
      "Extracted Aspects: ['service', 'waiter']\n",
      "------------------------------------------------------------\n",
      "Sentence: The dessert was heavenly, and the coffee was perfectly brewed.\n",
      "Extracted Aspects: ['dessert', 'coffee']\n",
      "------------------------------------------------------------\n",
      "Sentence: Portions were generous but the main dish lacked flavor.\n",
      "Extracted Aspects: ['portions', 'main dish']\n",
      "------------------------------------------------------------\n",
      "Sentence: The ambiance was cozy, and the music set the perfect mood.\n",
      "Extracted Aspects: ['am', '##biance', 'music']\n",
      "------------------------------------------------------------\n",
      "Sentence: The hotel room was spacious and clean, but the Wi-Fi connection was poor.\n",
      "Extracted Aspects: ['wi - fi connection']\n",
      "------------------------------------------------------------\n",
      "Sentence: I loved the guided tour, but the transportation was uncomfortable.\n",
      "Extracted Aspects: ['transportation']\n",
      "------------------------------------------------------------\n",
      "Sentence: The beach was pristine and beautiful, though a bit crowded.\n",
      "Extracted Aspects: ['beach']\n",
      "------------------------------------------------------------\n",
      "Sentence: The local food was delicious, but the prices were a bit high.\n",
      "Extracted Aspects: ['local', 'food', 'prices']\n",
      "------------------------------------------------------------\n",
      "Sentence: The museum had an impressive collection but lacked clear explanations.\n",
      "Extracted Aspects: []\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Evaluation / Prediction helper ---\n",
    "def predict_aspect_terms(sentence):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    encoding = tokenizer(sentence.split(), is_split_into_words=True,\n",
    "                         return_offsets_mapping=True, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predictions = predictions[0].cpu().numpy()\n",
    "\n",
    "    word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "    aspect_terms = []\n",
    "    current_term = []\n",
    "    for idx, pred in enumerate(predictions):\n",
    "        word_idx = word_ids[idx]\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "        label = pred\n",
    "        token = tokens[idx-1] if idx > 0 else tokens[idx]\n",
    "        if label == 1:  # B-ASP\n",
    "            if current_term:\n",
    "                aspect_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "                current_term = []\n",
    "            current_term = [token]\n",
    "        elif label == 2 and current_term:  # I-ASP\n",
    "            current_term.append(token)\n",
    "        else:\n",
    "            if current_term:\n",
    "                aspect_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "                current_term = []\n",
    "    # Add last term\n",
    "    if current_term:\n",
    "        aspect_terms.append(tokenizer.convert_tokens_to_string(current_term))\n",
    "\n",
    "    return aspect_terms\n",
    "\n",
    "# --- 11. Test predict ---\n",
    "test_sentences = [\n",
    "    # Chủ đề phim ảnh\n",
    "    \"The acting was phenomenal, but the storyline was predictable.\",\n",
    "    \"I loved the cinematography, but the pacing was too slow.\",\n",
    "    \"The soundtrack was amazing and the dialogue felt natural.\",\n",
    "    \"The plot twists were unexpected, but the ending was disappointing.\",\n",
    "    \"The characters were well-developed and very relatable.\",\n",
    "    \"The visual effects were stunning, but the script was weak.\",\n",
    "    \"The movie had a great message, but the direction lacked focus.\",\n",
    "    \"The performance by the lead actor was top-notch.\",\n",
    "    \"The editing was smooth and the scenes transitioned well.\",\n",
    "    \"The humor was forced, and the romance felt unnecessary.\",\n",
    "\n",
    "    # Chủ đề công nghệ\n",
    "    \"The battery life of this smartphone is incredible, but the camera quality is mediocre.\",\n",
    "    \"I really appreciate the fast processor, but the device heats up quickly.\",\n",
    "    \"The screen resolution is crystal clear, yet the speaker sound is disappointing.\",\n",
    "    \"Charging is fast but the charger cable feels fragile.\",\n",
    "    \"The software update fixed many bugs but introduced new ones.\",\n",
    "\n",
    "    # Chủ đề dịch vụ khách hàng\n",
    "    \"The customer service was very helpful and resolved my issue quickly.\",\n",
    "    \"Waiting time was too long, but the representative was polite.\",\n",
    "    \"They responded promptly, but the solution was not satisfactory.\",\n",
    "    \"The support team was unprofessional and rude.\",\n",
    "    \"I appreciated the follow-up calls after the purchase.\",\n",
    "\n",
    "    # Chủ đề ẩm thực\n",
    "    \"The pizza crust was crispy and delicious, but the toppings were sparse.\",\n",
    "    \"Service was quick, but the waiter forgot our drinks.\",\n",
    "    \"The dessert was heavenly, and the coffee was perfectly brewed.\",\n",
    "    \"Portions were generous but the main dish lacked flavor.\",\n",
    "    \"The ambiance was cozy, and the music set the perfect mood.\",\n",
    "\n",
    "    # Chủ đề du lịch\n",
    "    \"The hotel room was spacious and clean, but the Wi-Fi connection was poor.\",\n",
    "    \"I loved the guided tour, but the transportation was uncomfortable.\",\n",
    "    \"The beach was pristine and beautiful, though a bit crowded.\",\n",
    "    \"The local food was delicious, but the prices were a bit high.\",\n",
    "    \"The museum had an impressive collection but lacked clear explanations.\"\n",
    "]\n",
    "\n",
    "\n",
    "for sent in test_sentences:\n",
    "    aspects = predict_aspect_terms(sent)\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    print(f\"Extracted Aspects: {aspects}\")\n",
    "    print('-'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870eb030-b61f-4842-a827-8b74c57e7295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model\\\\tokenizer_config.json',\n",
       " './saved_model\\\\special_tokens_map.json',\n",
       " './saved_model\\\\vocab.txt',\n",
       " './saved_model\\\\added_tokens.json',\n",
       " './saved_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13a52b-bf57-49e9-8bac-3e81687c3f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
